**1. How to technical tools promise to "fair out" the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?

In the past decisions of walfares are done by social workers. In this case discriminiation may happen because of personal attitudes. So the technical tools are supposed to fix this issue: the algorithm would not care about race, skin color etc. It may have better performance, yet it is sometimes inaccurate and causes other problems. On one aspect it collect too much personal data which is loosely regulated; on the other hand it doesn't not collect enough data that it has its own bias (since it targets at the poor class because it only takes the country insurance data but not private insurance data.)



**2. Imagine, what could this (following quotes) mean in the widest sense? "The state doesn't need a cop to kill a person" and "electronic incarceration"

It means that the current welfare distrubutioln system based on data analysis cannot run properly sometimes and cause harm to the people in need. The audio lists the death of a women because she didn't not response the phone meeting due to illness. Also, this data analysis also implicitly contrtols and limits people's behavior like incarcernation, like the family would check the list and fulfill the corresponding measurements with care (and probably neglect what are not measured but vital to children), to prevent the government take away their kids. 



**3. "systems act as a kind of 'empathy-overwrite'"

When a certain decision of welfare distrubution is made by a human, he may be criticised by many for possile unfairness. Yet when the machine is making a decision, it sounds more logcal and convincing so that the public may accept the decision more and ignore the underlying the problems within it.



**4. China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges (recent example). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of "technical systems not well thought-through about what their impace on human beings is"

It is hard for me to find a "technical systems not well thought-through about what their impace on human beings is" since I feel most of the "negative impact" (especially in terms of privacy etc.) are purposedly designed. 
Anyways, in the time of virus many school uses online office software or live software to deliver lectures (like us :) ). Yet this way of teaching is also utilized in some school in the poor region. Some of the students their cannot afford a smart phone or laptop. There has been reports that one student hides on the roof of his neighbor's house to use their wifi as his family cannot afford a high speed one. There is another report that a girl lived in rural area suisided as she is the only one in her school that cannot afford the device and Internet that she feels so isolated. 
